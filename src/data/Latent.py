from abc import ABC, abstractmethod
import numpy as np
import tensorflow as tf


class Latent(ABC):
    """
    Abstract class for latent spaces.
    Subclasses only need to implement the sample() method.
    """

    @abstractmethod
    def __init__(self):
        """
        :param shape: The shape of a *single* sample.
        :param batch_size: The default batch size.
        """
        self.shape = None
        self.batch_size = None

    @abstractmethod
    def sample(self):
        """
        Returns several samples from the latent space.
        """
        pass

# This is the standard way to generate points. All points are generated by one Gaussian. With this latent space it takes
# much more time to train

class Gaussian_latent(Latent):

    def __init__(self, shape=None, batch_size=None):
        super().__init__()
        self.shape = shape
        self.batch_size = batch_size
        self.name = "Tensorflow_latent"
    def sample(self, batch_size=None):
        raise NotImplemented

    def tensor(self):
        return tf.random_normal(shape=(self.batch_size, self.shape))

# With this method, the points are taken from N Gaussians with some variance sigma.
# If one makes N too big and variance small then we have a ``perfect fit", or an overfit depending how one sees it.
# This is the case where we get the biggest amount of details in our dataset.
# If the number of Gaussians is small then it is harder to train and someone has to increase the number of critic steps and variance to avoid mode collapse.
# However we expect that if the dataset allows it, then small N of Gaussians will result in some clustering of the data.
# We also prefer this method from the standard one, because we believe that quite often the data manifold has different geometry in different parts.

# In the case of Fashion Mnist one needs more Gaussians than the dataset itself to achieve a very good approximation.
# We believe that this happens because the Mnist databases do not really have any accumulation points, at least with the Euclidean distance.
# If one takes a smaller number of Gaussians (80%-90% of the points) with medium variance (0.5),
# then we observe some interesting phenomena. The training method will start dropping some of the points (some mode collapse) but increase the accuracy.
# Further increase on the variance will result in a training similar to the case of the one Gaussian.
# A more dynamic way of changing the number of gaussian and assigning individual variances may result in better data representation.


class Assignment_latent(Latent):

    def __init__(self, shape=None, batch_size=None):
        super().__init__()
        N_Gaussians=10000
        self.shape = shape
        self.batch_size = batch_size
        self.initial_points=np.random.normal(0,1,(N_Gaussians,self.shape))
        self.fixed_latent = tf.constant(self.initial_points, dtype=tf.float32)
        self.name = "Assignment_latent"
        self.sigma=0.1
    def sample(self, batch_size=None):
        raise NotImplemented

    def tensor(self):
        a=tf.random_shuffle(self.fixed_latent, seed=None, name=None)
        a=tf.slice(a,[0,0],[self.batch_size,self.shape])
        return a+ self.sigma*tf.random_normal(shape=(self.batch_size, self.shape))

